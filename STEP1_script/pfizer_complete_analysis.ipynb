{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Pfizer Territory Optimization - STEP 1 Complete Analysis\n",
    "\n",
    "**Project:** Decision Modelling 2025-26  \n",
    "**Deadline:** December 18, 2024  \n",
    "**Problem:** Multi-objective optimization for Sales Representative territory assignment  \n",
    "\n",
    "---\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "- **4 Sales Representatives (SRs)** need to be assigned to **22 bricks (territories)**\n",
    "- **Two competing objectives:**\n",
    "  1. Minimize total travel distance\n",
    "  2. Minimize disruption to current assignments\n",
    "- **Constraints:**\n",
    "  - Each brick assigned to exactly one SR\n",
    "  - Workload balance: each SR gets between [wl_min, wl_max] workload\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading & Validation](#section1)\n",
    "2. [Current Assignment Analysis](#section2)\n",
    "3. [Model 1: Minimize Distance](#section3)\n",
    "4. [Model 2: Minimize Disruption](#section4)\n",
    "5. [Pareto Frontier Analysis](#section5)\n",
    "6. [Scenario Comparison](#section6)\n",
    "7. [Key Insights & Recommendations](#section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section1'></a>\n",
    "## 1. Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pfizer_optimization import PfizerOptimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and load data\n",
    "opt = PfizerOptimization(data_path='data/')\n",
    "opt.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nNumber of bricks: {len(opt.bricks)}\")\n",
    "print(f\"Number of SRs: {len(opt.srs)}\")\n",
    "print(f\"Total workload: {sum(opt.workload.values()):.4f}\")\n",
    "print(f\"Target per SR: {sum(opt.workload.values()) / len(opt.srs):.4f}\")\n",
    "\n",
    "print(\"\\nSR Office Locations (Center Bricks):\")\n",
    "for sr, brick in opt.center_bricks.items():\n",
    "    print(f\"  SR{sr}: Brick {brick}\")\n",
    "\n",
    "print(\"\\nWorkload Statistics:\")\n",
    "workload_df = pd.DataFrame(list(opt.workload.items()), columns=['Brick', 'Workload'])\n",
    "print(workload_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current_analysis_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2'></a>\n",
    "## 2. Current Assignment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current assignment\n",
    "current_sol = opt.analyze_current_assignment()\n",
    "opt.print_solution_comparison(current_sol, \"Current Territory Assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current_problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify workload violations\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"CURRENT ASSIGNMENT PROBLEMS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "violations = []\n",
    "for sr, wl in current_sol['workloads'].items():\n",
    "    if wl < 0.8:\n",
    "        violations.append(f\"SR{sr}: UNDERLOADED ({wl:.4f} < 0.8)\")\n",
    "    elif wl > 1.2:\n",
    "        violations.append(f\"SR{sr}: OVERLOADED ({wl:.4f} > 1.2)\")\n",
    "\n",
    "if violations:\n",
    "    print(\"\\nWorkload Constraint Violations:\")\n",
    "    for v in violations:\n",
    "        print(f\"  ✗ {v}\")\n",
    "else:\n",
    "    print(\"\\n✓ All workload constraints satisfied\")\n",
    "\n",
    "print(f\"\\nTotal Distance: {current_sol['total_distance']:.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model1_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section3'></a>\n",
    "## 3. Model 1: Minimize Distance\n",
    "\n",
    "**Objective:** Minimize total travel distance  \n",
    "**Constraints:**  \n",
    "- Each brick assigned to exactly one SR\n",
    "- Workload balance: 0.8 ≤ workload ≤ 1.2 for each SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model1_solve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Model 1\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: MINIMIZE DISTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "m1, sol1 = opt.model_1_minimize_distance(wl_min=0.8, wl_max=1.2, verbose=True)\n",
    "opt.print_solution_comparison(sol1, \"Model 1: Minimum Distance Solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model1_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "A = opt.create_current_assignment_matrix()\n",
    "disruption1 = opt._calculate_disruption(sol1, A)\n",
    "distance_improvement = current_sol['total_distance'] - sol1['total_distance']\n",
    "distance_pct = (distance_improvement / current_sol['total_distance']) * 100\n",
    "\n",
    "print(\"\\nModel 1 Performance Metrics:\")\n",
    "print(f\"  Total Distance: {sol1['total_distance']:.2f} km\")\n",
    "print(f\"  Distance Improvement: {distance_improvement:.2f} km ({distance_pct:.1f}%)\")\n",
    "print(f\"  Disruption: {disruption1:.4f}\")\n",
    "\n",
    "# Count reassignments\n",
    "reassignments = sum(1 for i in opt.bricks for j in opt.srs \n",
    "                   if i in sol1['assignment'][j] and A[(i, j)] == 0)\n",
    "print(f\"  Bricks Reassigned: {reassignments}/{len(opt.bricks)}\")\n",
    "print(f\"  Retention Rate: {((len(opt.bricks) - reassignments) / len(opt.bricks) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model2_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section4'></a>\n",
    "## 4. Model 2: Minimize Disruption\n",
    "\n",
    "**Objective:** Minimize disruption (weighted by workload)  \n",
    "**Disruption Metric:** Sum of index values for reassigned bricks  \n",
    "**Constraints:** Same as Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model2_solve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Model 2\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: MINIMIZE DISRUPTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "m2, sol2 = opt.model_2_minimize_disruption(wl_min=0.8, wl_max=1.2, verbose=True)\n",
    "opt.print_solution_comparison(sol2, \"Model 2: Minimum Disruption Solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model2_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "disruption2 = opt._calculate_disruption(sol2, A)\n",
    "reassignments2 = sum(1 for i in opt.bricks for j in opt.srs \n",
    "                    if i in sol2['assignment'][j] and A[(i, j)] == 0)\n",
    "\n",
    "print(\"\\nModel 2 Performance Metrics:\")\n",
    "print(f\"  Total Distance: {sol2['total_distance']:.2f} km\")\n",
    "print(f\"  Distance vs Current: {sol2['total_distance'] - current_sol['total_distance']:.2f} km\")\n",
    "print(f\"  Disruption: {disruption2:.4f}\")\n",
    "print(f\"  Bricks Reassigned: {reassignments2}/{len(opt.bricks)}\")\n",
    "print(f\"  Retention Rate: {((len(opt.bricks) - reassignments2) / len(opt.bricks) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Total Distance (km)', 'Disruption', 'Bricks Reassigned', 'Retention Rate (%)'],\n",
    "    'Current': [current_sol['total_distance'], 0.0, 0, 100.0],\n",
    "    'Model 1 (Min Dist)': [sol1['total_distance'], disruption1, reassignments, \n",
    "                           (len(opt.bricks) - reassignments) / len(opt.bricks) * 100],\n",
    "    'Model 2 (Min Disr)': [sol2['total_distance'], disruption2, reassignments2,\n",
    "                           (len(opt.bricks) - reassignments2) / len(opt.bricks) * 100]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section5'></a>\n",
    "## 5. Pareto Frontier Analysis\n",
    "\n",
    "Using the **epsilon-constraint method**, we generate the Pareto frontier:  \n",
    "- **Primary objective:** Minimize distance  \n",
    "- **Constraint:** Disruption ≤ ε (varying)  \n",
    "\n",
    "We analyze **3 workload scenarios:**\n",
    "1. **Scenario 1:** [0.8, 1.2] - Wide bounds (flexible)\n",
    "2. **Scenario 2:** [0.85, 1.15] - Medium bounds\n",
    "3. **Scenario 3:** [0.9, 1.1] - Tight bounds (strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed Pareto results\n",
    "with open('pareto_results.pkl', 'rb') as f:\n",
    "    pareto_data = pickle.load(f)\n",
    "\n",
    "pareto_results = pareto_data['pareto_frontiers']\n",
    "scenarios = pareto_data['scenarios']\n",
    "\n",
    "print(\"Loaded Pareto frontier data:\")\n",
    "for scenario_name, df in pareto_results.items():\n",
    "    print(f\"  {scenario_name}: {len(df)} solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "summary_df = pd.read_csv('pareto_summary.csv')\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARETO FRONTIER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Pareto comparison plot\n",
    "display(Image('pareto_comparison.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario1_detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1 detailed plot\n",
    "print(\"\\nScenario 1: [0.8, 1.2] - Detailed Analysis\")\n",
    "display(Image('pareto_Scenario_1_0.8_1.2.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario1_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some Pareto-optimal solutions for Scenario 1\n",
    "s1_df = pareto_results['Scenario 1: [0.8, 1.2]']\n",
    "print(\"\\nScenario 1: Sample Pareto-Optimal Solutions\")\n",
    "print(s1_df[['distance', 'disruption']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenario_comparison_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section6'></a>\n",
    "## 6. Scenario Comparison\n",
    "\n",
    "We compare the three workload scenarios to understand the impact of workload flexibility on solution quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workload_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display workload comparisons\n",
    "print(\"\\nWorkload Distribution Comparisons:\\n\")\n",
    "\n",
    "print(\"Scenario 1: [0.8, 1.2]\")\n",
    "display(Image('workload_Scenario_1_0.8_1.2.png'))\n",
    "\n",
    "print(\"\\nScenario 2: [0.85, 1.15]\")\n",
    "display(Image('workload_Scenario_2_0.85_1.15.png'))\n",
    "\n",
    "print(\"\\nScenario 3: [0.9, 1.1]\")\n",
    "display(Image('workload_Scenario_3_0.9_1.1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assignment_heatmaps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display assignment heatmaps for Scenario 1\n",
    "print(\"\\nAssignment Heatmaps (Scenario 1):\\n\")\n",
    "\n",
    "print(\"Min Distance Solution:\")\n",
    "display(Image('assignment_heatmap_scenario1_mindist.png'))\n",
    "\n",
    "print(\"\\nMin Disruption Solution:\")\n",
    "display(Image('assignment_heatmap_scenario1_mindisr.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trade-offs across scenarios\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for scenario_name, df in pareto_results.items():\n",
    "    min_dist = df['distance'].min()\n",
    "    max_dist = df['distance'].max()\n",
    "    min_disr = df['disruption'].min()\n",
    "    max_disr = df['disruption'].max()\n",
    "    \n",
    "    improvement = ((187.41 - min_dist) / 187.41) * 100\n",
    "    \n",
    "    print(f\"\\n{scenario_name}\")\n",
    "    print(f\"  Best Distance: {min_dist:.2f} km ({improvement:.1f}% improvement)\")\n",
    "    print(f\"  Worst Distance: {max_dist:.2f} km\")\n",
    "    print(f\"  Distance Range: {max_dist - min_dist:.2f} km\")\n",
    "    print(f\"  Min Disruption: {min_disr:.4f}\")\n",
    "    print(f\"  Max Disruption: {max_disr:.4f}\")\n",
    "    print(f\"  Pareto Solutions: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights_header",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section7'></a>\n",
    "## 7. Key Insights & Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### 1. Current Assignment Issues\n",
    "- **SR2 is overloaded:** 1.3377 > 1.2 (violation)\n",
    "- **SR3 is underloaded:** 0.7048 < 0.8 (violation)\n",
    "- **Total distance:** 187.41 km (not optimal)\n",
    "\n",
    "#### 2. Optimization Potential\n",
    "- **Model 1 (Min Distance):** Achieves 11.4% improvement with wide bounds [0.8, 1.2]\n",
    "- **Model 2 (Min Disruption):** Maintains stability but increases distance slightly\n",
    "- **Trade-off:** Distance improvements require some reassignments\n",
    "\n",
    "#### 3. Workload Flexibility Impact\n",
    "- **Wide bounds [0.8, 1.2]:** Better distance optimization (165.96 km)\n",
    "- **Tight bounds [0.9, 1.1]:** More balanced workload but higher distance (171.68 km)\n",
    "- **Difference:** ~6 km penalty for stricter workload balance\n",
    "\n",
    "#### 4. Pareto Frontier Characteristics\n",
    "- Multiple Pareto-optimal solutions available for decision-makers\n",
    "- Clear trade-off curve between distance and disruption\n",
    "- Scenario 1 offers most flexibility and best solutions\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Implement optimization:** Current assignment violates workload constraints\n",
    "2. **Choose scenario based on priorities:**\n",
    "   - If distance is critical → Use Scenario 1 solution\n",
    "   - If workload balance is critical → Use Scenario 3 solution\n",
    "   - If stability matters → Choose Pareto solution with low disruption\n",
    "3. **Consider Pareto middle ground:** Solutions with moderate distance improvement and low disruption\n",
    "4. **Next steps:** Implement partial brick assignments (STEP 2) for even better solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend a balanced solution from Scenario 1\n",
    "s1_df = pareto_results['Scenario 1: [0.8, 1.2]']\n",
    "\n",
    "# Find solution with good distance but low disruption\n",
    "# Normalize both objectives and find compromise\n",
    "s1_df['norm_dist'] = (s1_df['distance'] - s1_df['distance'].min()) / (s1_df['distance'].max() - s1_df['distance'].min())\n",
    "s1_df['norm_disr'] = (s1_df['disruption'] - s1_df['disruption'].min()) / (s1_df['disruption'].max() - s1_df['disruption'].min())\n",
    "s1_df['compromise'] = s1_df['norm_dist'] + s1_df['norm_disr']\n",
    "\n",
    "best_compromise_idx = s1_df['compromise'].idxmin()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED SOLUTION (Balanced Compromise)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nScenario: [0.8, 1.2]\")\n",
    "print(f\"Distance: {s1_df.loc[best_compromise_idx, 'distance']:.2f} km\")\n",
    "print(f\"Disruption: {s1_df.loc[best_compromise_idx, 'disruption']:.4f}\")\n",
    "print(f\"Improvement vs Current: {187.41 - s1_df.loc[best_compromise_idx, 'distance']:.2f} km\")\n",
    "print(f\"\\nThis solution offers a good balance between distance minimization and\")\n",
    "print(f\"maintaining stability (low disruption to current assignments).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**STEP 1 Complete:** We have successfully:\n",
    "- ✅ Implemented Model 1 (Minimize Distance)\n",
    "- ✅ Implemented Model 2 (Minimize Disruption)\n",
    "- ✅ Generated Pareto frontiers using epsilon-constraint method\n",
    "- ✅ Analyzed 3 workload scenarios: [0.8, 1.2], [0.85, 1.15], [0.9, 1.1]\n",
    "- ✅ Created visualizations and comprehensive analysis\n",
    "\n",
    "**Next Steps (STEP 2 & 3):**\n",
    "- Implement partial brick assignments (allow splitting bricks)\n",
    "- Optimize new SR office locations\n",
    "- Add center brick optimization (variable office locations)\n",
    "- Generate final report and presentation\n",
    "\n",
    "---\n",
    "\n",
    "**Files Generated:**\n",
    "- `pfizer_optimization.py` - Main optimization module\n",
    "- `pareto_analysis.py` - Complete Pareto analysis script\n",
    "- `pareto_results.pkl` - Pickled results for reuse\n",
    "- `pareto_*.csv` - Pareto frontier data\n",
    "- `pareto_*.png` - Pareto plots\n",
    "- `workload_*.png` - Workload comparisons\n",
    "- `assignment_heatmap_*.png` - Assignment visualizations\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
